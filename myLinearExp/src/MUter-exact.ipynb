{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05205fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from torchattacks import PGD\n",
    "\n",
    "from dataloder import *\n",
    "from argument import *\n",
    "from model import *\n",
    "from pretrain import *\n",
    "from utils import *\n",
    "from parllutils import *\n",
    "from modules import *\n",
    "\n",
    "args = argument()\n",
    "device = 'cuda'\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# random seed\n",
    "setup_seed(args.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9983f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_num = 600 # 共删除50个点\n",
    "delete_batch = 1 # 每次删1个点\n",
    "pass_batch = args.parllsize # batch_size 并行计算 total hessian\n",
    "\n",
    "muter_time1 = 0.0\n",
    "muter_time2_sequence = []\n",
    "muter_time3_sequence = []\n",
    "\n",
    "delete_num, delete_batch, pass_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87173240",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### 1) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fe55df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: tensor([1, 1, 1,  ..., 1, 7, 1])\n",
      "total number of train data: 13007, test data: 2163\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, re_sequence = Load_Data(args, delete_num, shuffle=True)\n",
    "train_loader = make_loader(train_data, batch_size=args.batchsize)\n",
    "test_loader = make_loader(test_data, batch_size=args.batchsize)\n",
    "print(f\"total number of train data: {len(train_data[0])}, test data: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dee6cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean test acc : 99.21%\n"
     ]
    }
   ],
   "source": [
    "lr, epochs, atk_info = training_param(args)\n",
    "\n",
    "# model\n",
    "model_clean = LogisticModel(input_featrue=get_featrue(args)).to(device)\n",
    "# loss function\n",
    "criterion = LossFunction(args.model).to(device)\n",
    "# setting optimizer\n",
    "optimizer = torch.optim.SGD(model_clean.parameters(), lr=lr)\n",
    "\n",
    "model_clean.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # pbar.set_description(desc)\n",
    "    # print('Epoch [{}/{}] training type {}, learning rate : {:.4f}'.format(epoch+1, epochs, args.adv, optimizer.param_groups[0]['lr']), end=' ')\n",
    "    total_loss = 0.0\n",
    "    step = 0\n",
    "    for data, label in train_loader:\n",
    "        label = label.to(device)\n",
    "        data = data.to(device)\n",
    "        output = model_clean(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        if args.lam != 0.0:\n",
    "            lam = torch.tensor(0.5 * args.lam)\n",
    "            l2_reg = torch.tensor(0.0)\n",
    "            for param in model_clean.parameters():\n",
    "                l2_reg = l2_reg + lam * param.pow(2.0).sum()\n",
    "            loss = loss + l2_reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        step = step + 1\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#     pbar.set_postfix(adv_train_type=args.adv, model=args.model, lr=optimizer.param_groups[0]['lr'], loss=total_loss/step, times=args.times)\n",
    "#     # print('loss : {:.5f}  adv_type : {} model : {}  times : {}'.format(total_loss/step, args.adv, args.model, args.times))\n",
    "#     time.sleep(0.1)\n",
    "\n",
    "\n",
    "model_clean.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# clean test acc\n",
    "for data, label in test_loader:\n",
    "    label = label.to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    predict = model_clean(data).round()\n",
    "    total = total + data.shape[0]\n",
    "    correct = correct + (predict == label).sum()\n",
    "\n",
    "clean_test_acc = float(correct) / total\n",
    "\n",
    "print('clean test acc : {:.2f}%'.format(clean_test_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c854362",
   "metadata": {},
   "source": [
    "### 2) load adversarially trained model (original model w*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e1e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model information: \n",
      "LogisticModel(\n",
      "  (fc): Linear(in_features=784, out_features=1, bias=False)\n",
      ")\n",
      "training type: PGD, epsilon: 0.25098, alpha: 0.03137, steps: 15\n",
      "training hyperparameters  lr: 0.010, epochs: 100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Adv Training: 100%|██████████| 100/100 [01:47<00:00,  1.08s/it, adv_train_type=PGD, loss=0.354, lr=0.01, model=logistic, times=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning PGD model spending 107.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticModel(\n",
       "   (fc): Linear(in_features=784, out_features=1, bias=False)\n",
       " ),\n",
       " 107.62872791290283)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adversarisal training\n",
    "# model_path = os.path.join('..', 'data', 'ATM', f\"dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_points_{len(train_loader.dataset)}_{args.times}.pth\")\n",
    "model, training_time = train(train_loader, test_loader, args, desc='Pre-Adv Training', verbose=True, model_path=None)\n",
    "model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcf0647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.22%\n"
     ]
    }
   ],
   "source": [
    "clean_acc, perturb_acc = Test_model(model, test_loader, args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b93c3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015443801879882812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 3,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 102,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7736af5fd35d4f02a5a137d470bb3979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-120426.9531,  -60783.2812,  -60783.3164,  ...,  -60783.3164,\n",
       "          -60783.2969,  -60783.2969],\n",
       "        [ -60783.2812, -120426.9297,  -60783.2969,  ...,  -60783.2969,\n",
       "          -60783.2930,  -60783.2969],\n",
       "        [ -60783.3164,  -60783.3047, -120427.0078,  ...,  -60783.3281,\n",
       "          -60783.3281,  -60783.3281],\n",
       "        ...,\n",
       "        [ -60783.3164,  -60783.3047,  -60783.3281,  ..., -120427.0391,\n",
       "          -60783.3281,  -60783.3281],\n",
       "        [ -60783.2969,  -60783.2969,  -60783.3281,  ...,  -60783.3281,\n",
       "         -120427.0391,  -60783.3242],\n",
       "        [ -60783.3047,  -60783.2969,  -60783.3281,  ...,  -60783.3281,\n",
       "          -60783.3242, -120427.0391]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pass_loader = make_loader(train_data, batch_size=pass_batch)\n",
    "# Calculate the hessian matrix of partial_dd\n",
    "matrices = dict(MUter=None)\n",
    "\n",
    "method = 'MUter'\n",
    "isDelta = False\n",
    "ssr = 'unperturbed'\n",
    "filename = f'dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_method_{method}_sample_{ssr}_{args.times}.pt'\n",
    "\n",
    "start_time = time.time()\n",
    "matrices[method] = parll_calculate_memory_matrix(model, pass_loader, args, method, isDelta)\n",
    "end_time = time.time()\n",
    "muter_time1 = end_time - start_time\n",
    "matrices[method]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec9f8c",
   "metadata": {},
   "source": [
    "## Stage II: Unlearning\n",
    "1) Inner level attack method;\n",
    "2) Calculate the public part partial_xx and partial_xx_inv for linear model;\n",
    "3) Init gradient information;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb539bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import PGD\n",
    "import copy\n",
    "from utils import cg_solve, derive_inv\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Inner level attack method\n",
    "_, _, atk_info = training_param(args)\n",
    "atk = PGD(model, atk_info[0], atk_info[1], atk_info[2], lossfun=LossFunction(args.model), lam=args.lam)\n",
    "\n",
    "# Calculate the public part partial_xx and partial_xx_inv for linear model\n",
    "feature = get_featrue(args)\n",
    "weight = vec_param(model.parameters()).detach()\n",
    "public_partial_xx = (weight.mm(weight.t())).detach()\n",
    "public_partial_xx_inv = derive_inv(public_partial_xx, method='Neumann', iter=args.iterneumann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b4dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 # record unlearning times\n",
    "## compare with removal list [1, 2, 3, 4, 5, ~1%, ~2%, ~3%, ~4%, ~5%] \n",
    "remove_list = None\n",
    "if args.dataset == 'binaryMnist':\n",
    "    remove_list = [1, 2, 3, 4, 5, 120, 240, 360, 480, 600]  # for mnist\n",
    "elif args.dataset == 'phishing':\n",
    "    remove_list = [1, 2, 3, 4, 5, 100, 200, 300, 400, 500]  # for phsihing\n",
    "elif args.dataset == 'madelon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 20, 40, 60, 80, 100]  # for madelon\n",
    "elif args.dataset == 'covtype':\n",
    "    remove_list = [1, 2, 3, 4, 5, 5000, 10000, 15000, 20000, 25000]\n",
    "elif args.dataset == 'epsilon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 4000, 8000, 12000, 16000, 20000]\n",
    "else:\n",
    "    remove_list = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]  # for splice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2925ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_hessian(x, y, weight, public_partial_xx, public_partial_xx_inv, args):\n",
    "    \n",
    "    x_size = x.shape[0]\n",
    "    weight_size = weight.shape[0]\n",
    "    \n",
    "    z = torch.sigmoid(y*(x.t().mm(weight)))\n",
    "    D = z * (1 - z)\n",
    "    \n",
    "    partial_ww = (D * (x.mm(x.t()))) + (args.lam * torch.eye(weight_size)).to(device)\n",
    "    partial_wx = (D * (x.mm(weight.t()))) + ((z-1) * y * torch.eye(x_size).to(device))\n",
    "    partial_xw = (D * (weight.mm(x.t()))) + ((z-1) * y * torch.eye(weight_size).to(device))\n",
    "    partial_xx = D * public_partial_xx\n",
    "    partial_xx_inv = (1/D) * public_partial_xx_inv\n",
    "    \n",
    "    return partial_ww.detach(), partial_wx.detach(), partial_xw.detach(), partial_xx.detach(), partial_xx_inv.detach()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7cf4a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1-th delete\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34626/595918282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_delete_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_delete_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mclean_grad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparll_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/privacyGroup/liujunxu/MUter_code/myLinearExp/src/torchattacks/attack.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgiven_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/privacyGroup/liujunxu/MUter_code/myLinearExp/src/torchattacks/attacks/pgd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# add l2 regular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/privacyGroup/liujunxu/MUter_code/myLinearExp/src/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output, label)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# ridge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3086\u001b[0m         raise ValueError(\n\u001b[1;32m   3087\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3088\u001b[0;31m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3089\u001b[0m         )\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Init gradinet informations\n",
    "grad = torch.zeros((feature, 1)).to(device)\n",
    "clean_grad = torch.zeros((feature, 1)).to(device)\n",
    "parll_partial = batch_indirect_hessian(args)\n",
    "\n",
    "# 从1开始删50个点\n",
    "for batch_delete_num in range(1, delete_num+1, 1):\n",
    "    print('The {}-th delete'.format(batch_delete_num))\n",
    "    # prepare work\n",
    "    unlearning_model = copy.deepcopy(model).to(device)  # for MUter method\n",
    "    x = train_data[0][batch_delete_num].to(device)\n",
    "    y = train_data[1][batch_delete_num].to(device)\n",
    "    x_delta = atk(x, y).to(device)\n",
    "    \n",
    "    clean_grad += parll_loss_grad(weight, x.view(1, feature), y, args).detach()\n",
    "    \n",
    "    # stage 2: compute delta_w\n",
    "    start_time = time.time()\n",
    "    ## Unlearning by Schur Complement Conversion\n",
    "    # Mr[nabla_w] + nabla_w_{r+1}\n",
    "    # .view(1, feature): single point\n",
    "    grad += parll_loss_grad(weight, x_delta.view(1, feature), y, args).detach()  \n",
    "#     print('updated_grad: ', grad.view(1, feature))\n",
    "    partial_ww, partial_wx, partial_xw, partial_xx, partial_xx_inv = partial_hessian(x_delta.view(feature, 1), y, weight, public_partial_xx, public_partial_xx_inv, args)\n",
    "    \n",
    "    block_matrix = buliding_matrix(matrices['MUter'], partial_ww, partial_wx, -partial_xx, partial_xw)\n",
    "    grad_cat_zero = torch.cat([grad, torch.zeros((feature, 1)).to(device)], dim=0)\n",
    "    \n",
    "    delta_w_cat_alpha = cg_solve(block_matrix, grad_cat_zero.squeeze(dim=1), get_iters(args))\n",
    "    delta_w = delta_w_cat_alpha[:feature]\n",
    "#     print('delta_w:', delta_w)\n",
    "    update_w(delta_w, unlearning_model)\n",
    "    \n",
    "    clean_acc, perturb_acc = Test_model(unlearning_model, test_loader, args) \n",
    "    print(clean_acc, perturb_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    muter_time2 = end_time - start_time\n",
    "    muter_time2_sequence.append(muter_time2)\n",
    "    \n",
    "    # stage 3: update matrix\n",
    "    start_time = time.time()\n",
    "    ## update matrix M_{r+1}\n",
    "    matrices['MUter'] -= (partial_ww - (partial_wx.mm(partial_xx_inv.mm(partial_xw))))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    muter_time3 = end_time - start_time\n",
    "    muter_time3_sequence.append(muter_time3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cff9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('avg stage 1 time {:.4f}'.format(np.mean(muter_time1)))\n",
    "print('avg stage 2 time {:.4f}'.format(np.mean(muter_time2_sequence)))\n",
    "print('avg stage 3 time {:.4f}'.format(np.mean(muter_time3_sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608dfc92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
