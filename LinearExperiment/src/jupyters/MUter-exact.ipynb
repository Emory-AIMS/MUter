{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e46900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(adv='PGD', batchsize=128, dataset='binaryMnist', deletebatch=1, deletenum=0, isbatch=False, iterneumann=3, lam=0.0001, model='logistic', parllsize=128, remove_type=2, times=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from torchattacks import PGD\n",
    "\n",
    "from dataloder import *\n",
    "from argument import *\n",
    "from model import *\n",
    "from pretrain import *\n",
    "from utils import *\n",
    "from parllutils import *\n",
    "from modules import *\n",
    "\n",
    "args = argument()\n",
    "device = 'cuda'\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# random seed\n",
    "setup_seed(args.times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7c2c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_num = 50 # 共删除50个点\n",
    "delete_batch = 1 # 每次删1个点\n",
    "pass_batch = args.parllsize # batch_size 并行计算 total hessian\n",
    "delete_num, delete_batch, pass_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecbb83",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### 1) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d67b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: tensor([1, 1, 1,  ..., 1, 7, 1])\n",
      "total number of train data: 13007, test data: 2163\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, re_sequence = Load_Data(args, delete_num, shuffle=True)\n",
    "train_loader = make_loader(train_data, batch_size=args.batchsize)\n",
    "test_loader = make_loader(test_data, batch_size=args.batchsize)\n",
    "print(f\"total number of train data: {len(train_data[0])}, test data: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05e6f5",
   "metadata": {},
   "source": [
    "### 2) load adversarially trained model (original model w*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113d6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarisal training\n",
    "model_path = os.path.join('..', 'data', 'ATM', f\"dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_points_{len(train_loader.dataset)}_{args.times}.pth\")\n",
    "model, training_time = train(train_loader, test_loader, args, desc='Pre-Adv Training', verbose=True, model_path=model_path)\n",
    "model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3cd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_MUter_sample_unperturbed.pt\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014536380767822266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 13,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 102,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833633a34a964a92b69d2c514f1551c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "memory matrix for MUter method using un-perturb samples to calculate\n",
      "saving matrix...\n",
      "done!\n",
      "MUter\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Newton_sample_perturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Newton_delta\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Fisher_sample_perturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Fisher_delta\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Influence_sample_perturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Influence_delta\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Newton_sample_unperturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Newton\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Fisher_sample_unperturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Fisher\n",
      "loading memory matrix : ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_Influence_sample_unperturbed.pt\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "done!\n",
      "Influence\n"
     ]
    }
   ],
   "source": [
    "pass_loader = make_loader(train_data, batch_size=pass_batch)\n",
    "# Calculate the hessian matrix of partial_dd\n",
    "matrices = dict(MUter=None)\n",
    "\n",
    "method = 'MUter'\n",
    "isDelta = False\n",
    "ssr = 'unperturbed'\n",
    "filename = f'dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_method_{method[0]}_sample_{ssr}_{args.times}.pt'\n",
    "print(name)\n",
    "\n",
    "start_time = time.time()\n",
    "matrices[name] = load_memory_matrix(filename, model, pass_loader, method, isDelta, args)\n",
    "end_time = time.time()\n",
    "muter_time1 = start_time - end_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b1fb9",
   "metadata": {},
   "source": [
    "## Stage II: Unlearning\n",
    "1) Inner level attack method;\n",
    "2) Calculate the public part partial_xx and partial_xx_inv for linear model;\n",
    "3) Init gradient information;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import PGD\n",
    "import copy\n",
    "from utils import cg_solve, model_distance, hessian, update_w, derive_inv\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Inner level attack method\n",
    "_, _, atk_info = training_param(args)\n",
    "atk = PGD(model, atk_info[0], atk_info[1], atk_info[2], lossfun=LossFunction(args.model), lam=args.lam)\n",
    "\n",
    "# Calculate the public part partial_xx and partial_xx_inv for linear model\n",
    "feature = get_featrue(args)\n",
    "weight = vec_param(model.parameters()).detach()\n",
    "public_partial_dd = (weight.mm(weight.t())).detach()\n",
    "public_partial_dd_inv = derive_inv(public_partial_dd, method='Neumann', iter=args.iterneumann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 # record unlearning times\n",
    "## compare with removal list [1, 2, 3, 4, 5, ~1%, ~2%, ~3%, ~4%, ~5%] \n",
    "remove_list = None\n",
    "if args.dataset == 'binaryMnist':\n",
    "    remove_list = [1, 2, 3, 4, 5, 120, 240, 360, 480, 600]  # for mnist\n",
    "elif args.dataset == 'phishing':\n",
    "    remove_list = [1, 2, 3, 4, 5, 100, 200, 300, 400, 500]  # for phsihing\n",
    "elif args.dataset == 'madelon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 20, 40, 60, 80, 100]  # for madelon\n",
    "elif args.dataset == 'covtype':\n",
    "    remove_list = [1, 2, 3, 4, 5, 5000, 10000, 15000, 20000, 25000]\n",
    "elif args.dataset == 'epsilon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 4000, 8000, 12000, 16000, 20000]\n",
    "else:\n",
    "    remove_list = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]  # for splice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc03981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init gradinet informations\n",
    "grad = torch.zeros((feature, 1)).to(device)\n",
    "clean_grad = torch.zeros((feature, 1)).to(device)\n",
    "parll_partial = batch_indirect_hessian(args)\n",
    "\n",
    "# 从1开始删600个点\n",
    "for batch_delete_num in range(1, delete_num+1, 1):\n",
    "    print('The {}-th delete'.format(batch_delete_num))\n",
    "    # prepare work\n",
    "    unlearning_model = copy.deepcopy(model).to(device)  # for MUter method\n",
    "    x = train_data[0][batch_delete_num].to(device)\n",
    "    y = train_data[1][batch_delete_num].to(device)\n",
    "    x_delta = atk(x, y).to(device)\n",
    "\n",
    "    update_grad(grad, clean_grad, weight, x, x_delta, y, feature, args)\n",
    "    \n",
    "    H_11, H_12, H_21, neg_H_22 = None, None, None, None\n",
    "    \n",
    "    Dww = partial_ww - (partial_wx.mm(partial_xx_inv.mm(partial_xw)))\n",
    "    \n",
    "    MUter(batch_delete_num, delete_loader, matrix, grad, unlearning_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695644a0",
   "metadata": {},
   "source": [
    "# Unlearning methods.\n",
    "## MUter stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage2(H_11, H_12, H_21, neg_H_22):\n",
    "    \n",
    "    z = torch.sigmoid(y*(x.t().mm(weight)))\n",
    "    D = z * (1 - z)\n",
    "    partial_ww = (D * (x.mm(x.t()))) + (args.lam * torch.eye(weight_size)).to(device)\n",
    "    partial_wx = (D * (x.mm(weight.t()))) + ((z-1) * y * torch.eye(x_size).to(device))\n",
    "    partial_xx_inv = (1/D) * public_partial_xx_inv\n",
    "    #partial_xx_inv = D * public_partial_xx_inv # to verify is right\n",
    "    partial_xw = (D * (weight.mm(x.t()))) + ((z-1) * y * torch.eye(weight_size).to(device))\n",
    "    public_partial_xx = public_partial_xx.to(device)\n",
    "    \n",
    "    partial_xx = D * public_partial_xx\n",
    "    return partial_ww.detach(), partial_wx.detach(), partial_xx_inv.detach(), partial_xw.detach(), partial_xx.detach()\n",
    "        \n",
    "    H_11, H_12, _, H_21, neg_H_22\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c33d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_partial_hessian(x, y, weight, public_partial_xx_inv):\n",
    "    \"\"\"\n",
    "    for loss function == 'logistic'\n",
    "    calculate single sample's partial_hessian, then using vamp function to \n",
    "    implement parll\n",
    "    \"\"\"\n",
    "    device = 'cuda'\n",
    "    size = weight.shape[0]\n",
    "\n",
    "    z = torch.sigmoid(y * (x.t().mm(weight)))\n",
    "    D = z * (1 - z)\n",
    "    partial_wx = (D * (x.mm(weight.t()))) + ((z-1) * y * torch.eye(size).to(device))\n",
    "    partial_xx_inv = (1/D) * public_partial_xx_inv\n",
    "    partial_xw = (D * (weight.mm(x.t()))) + ((z-1) * y * torch.eye(size).to(device))\n",
    "    \n",
    "    return  partial_wx.mm(partial_xx_inv.mm(partial_xw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea526b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MUter(batch_delete_num, delete_loader, matrix, grad, unlearning_model):\n",
    "    unlearning_time = 0.0 # record one batch spending time for MUter\n",
    "    # building matrix\n",
    "    Dww = None\n",
    "    H_11 = None\n",
    "    H_12 = None\n",
    "    H_21 = None\n",
    "    neg_H_22 = None\n",
    "    \n",
    "    # Unlearning\n",
    "    start_time = time.time()\n",
    "    for index, (image, label) in enumerate(delete_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        image_perturbed = atk(image, label).to(device)\n",
    "\n",
    "        if args.isbatch ==  False: # 删除单点：\n",
    "            Dww, H_11, H_12, _, H_21, neg_H_22 = partial_hessian(image_perturbed.view(feature, 1), label, \n",
    "                                                                 weight, public_partial_dd_inv, \n",
    "                                                                 args, isUn_inv=True, \n",
    "                                                                 public_partial_xx=public_partial_dd)    \n",
    "        else: # for mini-batch # 删除多点\n",
    "            matrix = matrix - \\\n",
    "                (batch_hessian(weight, image_perturbed.view(image_perturbed.shape[0], feature), label, args) - \\\n",
    "                 parll_partial(image_perturbed.view(image_perturbed.shape[0], feature, 1), label, \\\n",
    "                               weight, public_partial_dd_inv).sum(dim=0).detach())\n",
    "\n",
    "        grad = grad + \\\n",
    "            parll_loss_grad(weight, \\\n",
    "                            image_perturbed.view(image_perturbed.shape[0], feature), \\\n",
    "                            label, args)\n",
    "\n",
    "    if args.isbatch == False:\n",
    "        block_matrix = buliding_matrix(matrix, H_11, H_12, -neg_H_22, H_21)\n",
    "        print('block_matrix shape {}'.format(block_matrix.shape))\n",
    "        grad_cat_zero = torch.cat([grad, torch.zeros((feature, 1)).to(device)], dim=0)\n",
    "        print('grad_cat_zeor shape {}'.format(grad_cat_zero.shape))\n",
    "\n",
    "        delta_w_cat_alpha = cg_solve(block_matrix, grad_cat_zero.squeeze(dim=1), get_iters(args))\n",
    "        delta_w = delta_w_cat_alpha[:feature]\n",
    "\n",
    "        update_w(delta_w, unlearning_model)\n",
    "        matrix = matrix - Dww\n",
    "    else:\n",
    "        delta_w = cg_solve(matrix, grad.squeeze(dim=1), get_iters(args))\n",
    "        update_w(delta_w, unlearning_model)\n",
    "    \n",
    "    clean_acc, perturb_acc = Test_model(unlearning_model, test_loader, args) \n",
    "    model_dist = model_distance(retrain_model, unlearning_model)\n",
    "    print()\n",
    "    print('MUter unlearning:')\n",
    "    print(f'unlearning model test acc: clean_acc {clean_acc}, preturb_acc: {perturb_acc}')\n",
    "    print('model distance between Muter and retrain_from_scratch: {:.4f}'.format(model_dist))\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
