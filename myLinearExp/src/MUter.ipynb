{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9fa8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(adv='PGD', batchsize=128, dataset='binaryMnist', deletebatch=1, deletenum=0, isbatch=False, iterneumann=3, lam=0.0001, model='logistic', parllsize=128, remove_type=2, times=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from dataloder import *\n",
    "from argument import *\n",
    "from model import *\n",
    "from pretrain import *\n",
    "from utils import *\n",
    "from parllutils import *\n",
    "from functorch import vmap\n",
    "args = argument()\n",
    "device = 'cuda'\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89e2775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# random seed\n",
    "setup_seed(args.times)\n",
    "\n",
    "delete_num = 600 # 共删除600个点\n",
    "delete_batch = 1 # 每次删1个点\n",
    "pass_batch = args.parllsize # batch_size 并行计算 total hessian\n",
    "delete_num, delete_batch, pass_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6c14d",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### 1) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4feb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: tensor([1, 1, 1,  ..., 1, 7, 1])\n",
      "total number of train data: 13007, test data: 2163\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, re_sequence = Load_Data(args, delete_num, shuffle=True)\n",
    "train_loader = make_loader(train_data, batch_size=args.batchsize)\n",
    "test_loader = make_loader(test_data, batch_size=args.batchsize)\n",
    "print(f\"total number of train data: {len(train_data[0])}, test data: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d950c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13007 2163\n",
      "13007 2163\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[1]), len(test_data[1]))\n",
    "print(len(train_loader.dataset), len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4c24a",
   "metadata": {},
   "source": [
    "### 2) load adversarially trained model (original model w*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c68a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model information: \n",
      "LogisticModel(\n",
      "  (fc): Linear(in_features=784, out_features=1, bias=False)\n",
      ")\n",
      "training type: PGD, epsilon: 0.25098, alpha: 0.03137, steps: 15\n",
      "training hyperparameters  lr: 0.010, epochs: 100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-Adv Training: 100%|██████████| 100/100 [01:42<00:00,  1.03s/it, adv_train_type=PGD, loss=0.354, lr=0.01, model=logistic, times=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning PGD model spending 102.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticModel(\n",
       "   (fc): Linear(in_features=784, out_features=1, bias=False)\n",
       " ),\n",
       " 102.70159816741943)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adversarisal training  \n",
    "model_path = os.path.join('..', 'data', 'ATM', f\"dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_points_{len(train_loader.dataset)}_{args.times}.pth\")\n",
    "model, training_time = train(train_loader, test_loader, args, desc='Pre-Adv Training', verbose=True, model_path=model_path)\n",
    "model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03256165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# test the consistency among all trails with same augment `args.times`\n",
    "print(next(iter(test_loader))[1].reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e504aa",
   "metadata": {},
   "source": [
    "## 3) pre-unlearning: calculate the hessian matrix of partial_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a6d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading memory matrix of MUter method from: ../data/MemoryMatrix/dataset_binaryMnist_adv_PGD_model_logistic_method_MUter_sample_unperturbed_0.pt.pt\n",
      "======== matrix info ========\n",
      "tensor([[-181.3839,   -6.7562,   -7.1364,  ...,   -7.0635,   -7.2264,\n",
      "           -7.0113],\n",
      "        [  -6.7562, -181.4017,   -7.3416,  ...,   -7.0091,   -7.3634,\n",
      "           -7.0389],\n",
      "        [  -7.1364,   -7.3416, -182.2356,  ...,   -7.1665,   -7.6303,\n",
      "           -7.6300],\n",
      "        ...,\n",
      "        [  -7.0635,   -7.0091,   -7.1665,  ..., -181.1368,   -7.2876,\n",
      "           -7.0035],\n",
      "        [  -7.2264,   -7.3634,   -7.6303,  ...,   -7.2876, -182.3219,\n",
      "           -7.5114],\n",
      "        [  -7.0113,   -7.0389,   -7.6300,  ...,   -7.0035,   -7.5114,\n",
      "         -181.9788]], device='cuda:0')\n",
      "matrix shape torch.Size([784, 784]), type <class 'torch.Tensor'>\n",
      "MUter\n",
      "tensor([[-181.3839,   -6.7562,   -7.1364,  ...,   -7.0635,   -7.2264,\n",
      "           -7.0113],\n",
      "        [  -6.7562, -181.4017,   -7.3416,  ...,   -7.0091,   -7.3634,\n",
      "           -7.0389],\n",
      "        [  -7.1364,   -7.3416, -182.2356,  ...,   -7.1665,   -7.6303,\n",
      "           -7.6300],\n",
      "        ...,\n",
      "        [  -7.0635,   -7.0091,   -7.1665,  ..., -181.1368,   -7.2876,\n",
      "           -7.0035],\n",
      "        [  -7.2264,   -7.3634,   -7.6303,  ...,   -7.2876, -182.3219,\n",
      "           -7.5114],\n",
      "        [  -7.0113,   -7.0389,   -7.6300,  ...,   -7.0035,   -7.5114,\n",
      "         -181.9788]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pass_loader = make_loader(train_data, batch_size=pass_batch)\n",
    "matrices = dict(MUter=None,\n",
    "#                 Newton_delta=None,\n",
    "#                 Fisher_delta=None,\n",
    "#                 Influence_delta=None,\n",
    "#                 Newton=None,\n",
    "#                 Fisher=None,\n",
    "#                 Influence=None\n",
    "               )\n",
    "\n",
    "for name in matrices.keys():\n",
    "    method = name.split('_')\n",
    "    isDelta = True if len(method) > 1 else False\n",
    "    ssr = 'perturbed' if len(method) > 1 else 'unperturbed'\n",
    "    filename = f'dataset_{args.dataset}_adv_{args.adv}_model_{args.model}_method_{method[0]}_sample_{ssr}_{args.times}.pt'\n",
    "    matrices[name] = load_memory_matrix(filename, model, pass_loader, method[0],isDelta, args)\n",
    "    \n",
    "            \n",
    "for name in matrices.keys():\n",
    "    print(name)\n",
    "    print(matrices[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf1d6e",
   "metadata": {},
   "source": [
    "## Stage II: Unlearning\n",
    "1) Inner level attack method;\n",
    "2) Calculate the public part partial_xx and partial_xx_inv for linear model;\n",
    "3) Init gradient information;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e3af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticModel(\n",
      "  (fc): Linear(in_features=784, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchattacks import PGD\n",
    "import copy\n",
    "from utils import derive_inv\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Inner level attack method\n",
    "_, _, atk_info = training_param(args)\n",
    "print(model)\n",
    "atk = PGD(model, atk_info[0], atk_info[1], atk_info[2], lossfun=LossFunction(args.model), lam=args.lam)\n",
    "\n",
    "# Calculate the public part partial_xx and partial_xx_inv for linear model\n",
    "feature = get_featrue(args)\n",
    "weight = vec_param(model.parameters()).detach()\n",
    "public_partial_dd = (weight.mm(weight.t())).detach()\n",
    "public_partial_dd_inv = derive_inv(public_partial_dd, method='Neumann', iter=args.iterneumann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a771e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1 # record unlearning times\n",
    "## compare with removal list [1, 2, 3, 4, 5, ~1%, ~2%, ~3%, ~4%, ~5%] \n",
    "remove_list = None\n",
    "if args.dataset == 'binaryMnist':\n",
    "    remove_list = [1, 2, 3, 4, 5, 120, 240, 360, 480, 600]  # for mnist\n",
    "elif args.dataset == 'phishing':\n",
    "    remove_list = [1, 2, 3, 4, 5, 100, 200, 300, 400, 500]  # for phsihing\n",
    "elif args.dataset == 'madelon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 20, 40, 60, 80, 100]  # for madelon\n",
    "elif args.dataset == 'covtype':\n",
    "    remove_list = [1, 2, 3, 4, 5, 5000, 10000, 15000, 20000, 25000]\n",
    "elif args.dataset == 'epsilon':\n",
    "    remove_list = [1, 2, 3, 4, 5, 4000, 8000, 12000, 16000, 20000]\n",
    "else:\n",
    "    remove_list = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]  # for splice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546e512",
   "metadata": {},
   "source": [
    "## Unlearning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 deleted. label of cur image: 0.\n",
      "\n",
      "The 6-th delete\n",
      "delete_loader: train_data[0,1]\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.22%\n",
      "\n",
      "Retrain from scratch:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9122\n",
      "retrain time: 94.8896\n",
      "\n",
      "black matrix A shape torch.Size([1568, 1568]), type <class 'torch.Tensor'>\n",
      "Update done !\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.22%\n",
      "\n",
      "MUter unlearning:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9122\n",
      "model norm distance: 0.0786\n",
      "unlearning time: 0.0153\n",
      "\n",
      "2 deleted. label of cur image: 0.\n",
      "\n",
      "The 7-th delete\n",
      "delete_loader: train_data[1,2]\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.17%\n",
      "\n",
      "Retrain from scratch:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9117\n",
      "retrain time: 100.5480\n",
      "\n",
      "black matrix A shape torch.Size([1568, 1568]), type <class 'torch.Tensor'>\n",
      "Update done !\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.26%\n",
      "\n",
      "MUter unlearning:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9126\n",
      "model norm distance: 0.0734\n",
      "unlearning time: 0.0124\n",
      "\n",
      "3 deleted. label of cur image: 1.\n",
      "\n",
      "The 8-th delete\n",
      "delete_loader: train_data[2,3]\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.17%\n",
      "\n",
      "Retrain from scratch:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9117\n",
      "retrain time: 100.9030\n",
      "\n",
      "black matrix A shape torch.Size([1568, 1568]), type <class 'torch.Tensor'>\n",
      "Update done !\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.26%\n",
      "\n",
      "MUter unlearning:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9126\n",
      "model norm distance: 0.0645\n",
      "unlearning time: 0.0122\n",
      "\n",
      "4 deleted. label of cur image: 0.\n",
      "\n",
      "The 9-th delete\n",
      "delete_loader: train_data[3,4]\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.12%\n",
      "\n",
      "Retrain from scratch:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9112\n",
      "retrain time: 99.9078\n",
      "\n",
      "black matrix A shape torch.Size([1568, 1568]), type <class 'torch.Tensor'>\n",
      "Update done !\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.49%\n",
      "\n",
      "MUter unlearning:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9149\n",
      "model norm distance: 0.0692\n",
      "unlearning time: 0.0122\n",
      "\n",
      "5 deleted. label of cur image: 0.\n",
      "\n",
      "The 10-th delete\n",
      "delete_loader: train_data[4,5]\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.35%\n",
      "\n",
      "Retrain from scratch:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9135\n",
      "retrain time: 98.0954\n",
      "\n",
      "black matrix A shape torch.Size([1568, 1568]), type <class 'torch.Tensor'>\n",
      "Update done !\n",
      "clean test acc : 96.90%\n",
      "perturb test acc : 91.22%\n",
      "\n",
      "MUter unlearning:\n",
      "model test acc: clean_acc 0.9690, preturb_acc: 0.9122\n",
      "model norm distance: 0.0707\n",
      "unlearning time: 0.0122\n",
      "\n",
      "6 deleted. label of cur image: 0.\n",
      "7 deleted. label of cur image: 1.\n",
      "8 deleted. label of cur image: 0.\n",
      "9 deleted. label of cur image: 1.\n",
      "10 deleted. label of cur image: 1.\n",
      "11 deleted. label of cur image: 0.\n",
      "12 deleted. label of cur image: 0.\n",
      "13 deleted. label of cur image: 1.\n",
      "14 deleted. label of cur image: 0.\n",
      "15 deleted. label of cur image: 0.\n",
      "16 deleted. label of cur image: 0.\n",
      "17 deleted. label of cur image: 1.\n",
      "18 deleted. label of cur image: 0.\n",
      "19 deleted. label of cur image: 1.\n",
      "20 deleted. label of cur image: 0.\n",
      "21 deleted. label of cur image: 1.\n",
      "22 deleted. label of cur image: 0.\n",
      "23 deleted. label of cur image: 1.\n",
      "24 deleted. label of cur image: 0.\n",
      "25 deleted. label of cur image: 0.\n",
      "26 deleted. label of cur image: 1.\n",
      "27 deleted. label of cur image: 1.\n",
      "28 deleted. label of cur image: 1.\n",
      "29 deleted. label of cur image: 0.\n",
      "30 deleted. label of cur image: 0.\n",
      "31 deleted. label of cur image: 0.\n",
      "32 deleted. label of cur image: 0.\n",
      "33 deleted. label of cur image: 1.\n",
      "34 deleted. label of cur image: 0.\n",
      "35 deleted. label of cur image: 1.\n",
      "36 deleted. label of cur image: 1.\n",
      "37 deleted. label of cur image: 0.\n",
      "38 deleted. label of cur image: 0.\n",
      "39 deleted. label of cur image: 0.\n",
      "40 deleted. label of cur image: 0.\n",
      "41 deleted. label of cur image: 0.\n",
      "42 deleted. label of cur image: 0.\n",
      "43 deleted. label of cur image: 0.\n",
      "44 deleted. label of cur image: 1.\n",
      "45 deleted. label of cur image: 1.\n",
      "46 deleted. label of cur image: 1.\n",
      "47 deleted. label of cur image: 1.\n",
      "48 deleted. label of cur image: 1.\n",
      "49 deleted. label of cur image: 1.\n",
      "50 deleted. label of cur image: 1.\n",
      "51 deleted. label of cur image: 1.\n",
      "52 deleted. label of cur image: 1.\n",
      "53 deleted. label of cur image: 0.\n",
      "54 deleted. label of cur image: 0.\n",
      "55 deleted. label of cur image: 0.\n",
      "56 deleted. label of cur image: 0.\n",
      "57 deleted. label of cur image: 1.\n",
      "58 deleted. label of cur image: 1.\n",
      "59 deleted. label of cur image: 0.\n",
      "60 deleted. label of cur image: 1.\n",
      "61 deleted. label of cur image: 0.\n",
      "62 deleted. label of cur image: 0.\n",
      "63 deleted. label of cur image: 0.\n",
      "64 deleted. label of cur image: 0.\n",
      "65 deleted. label of cur image: 1.\n",
      "66 deleted. label of cur image: 0.\n",
      "67 deleted. label of cur image: 1.\n",
      "68 deleted. label of cur image: 0.\n",
      "69 deleted. label of cur image: 1.\n",
      "70 deleted. label of cur image: 1.\n",
      "71 deleted. label of cur image: 1.\n",
      "72 deleted. label of cur image: 1.\n",
      "73 deleted. label of cur image: 0.\n",
      "74 deleted. label of cur image: 0.\n",
      "75 deleted. label of cur image: 1.\n",
      "76 deleted. label of cur image: 1.\n",
      "77 deleted. label of cur image: 1.\n",
      "78 deleted. label of cur image: 1.\n",
      "79 deleted. label of cur image: 0.\n",
      "80 deleted. label of cur image: 1.\n",
      "81 deleted. label of cur image: 0.\n",
      "82 deleted. label of cur image: 1.\n",
      "83 deleted. label of cur image: 1.\n",
      "84 deleted. label of cur image: 1.\n",
      "85 deleted. label of cur image: 1.\n",
      "86 deleted. label of cur image: 0.\n",
      "87 deleted. label of cur image: 0.\n",
      "88 deleted. label of cur image: 0.\n",
      "89 deleted. label of cur image: 0.\n",
      "90 deleted. label of cur image: 0.\n",
      "91 deleted. label of cur image: 1.\n",
      "92 deleted. label of cur image: 0.\n",
      "93 deleted. label of cur image: 1.\n",
      "94 deleted. label of cur image: 0.\n",
      "95 deleted. label of cur image: 0.\n",
      "96 deleted. label of cur image: 1.\n",
      "97 deleted. label of cur image: 0.\n",
      "98 deleted. label of cur image: 0.\n",
      "99 deleted. label of cur image: 1.\n",
      "100 deleted. label of cur image: 0.\n",
      "101 deleted. label of cur image: 1.\n",
      "102 deleted. label of cur image: 0.\n",
      "103 deleted. label of cur image: 0.\n",
      "104 deleted. label of cur image: 1.\n",
      "105 deleted. label of cur image: 0.\n",
      "106 deleted. label of cur image: 1.\n",
      "107 deleted. label of cur image: 1.\n",
      "108 deleted. label of cur image: 0.\n",
      "109 deleted. label of cur image: 0.\n",
      "110 deleted. label of cur image: 0.\n",
      "111 deleted. label of cur image: 1.\n",
      "112 deleted. label of cur image: 0.\n",
      "113 deleted. label of cur image: 0.\n",
      "114 deleted. label of cur image: 1.\n",
      "115 deleted. label of cur image: 1.\n",
      "116 deleted. label of cur image: 0.\n",
      "117 deleted. label of cur image: 1.\n",
      "118 deleted. label of cur image: 0.\n",
      "119 deleted. label of cur image: 1.\n",
      "120 deleted. label of cur image: 0.\n",
      "temp_loader: train_data[5,119]\n",
      "\n",
      "The 11-th delete\n",
      "delete_loader: train_data[119,120]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-Adv Training:  23%|██▎       | 23/100 [00:22<01:13,  1.05it/s, adv_train_type=PGD, loss=0.395, lr=0.01, model=logistic, times=0]"
     ]
    }
   ],
   "source": [
    "from modules import *\n",
    "# Init gradinet informations\n",
    "grad = torch.zeros((feature, 1)).to(device)\n",
    "clean_grad = torch.zeros((feature, 1)).to(device)\n",
    "saver = None\n",
    "\n",
    "for batch_delete_num in range(delete_batch, delete_num+1, delete_batch):\n",
    "#     if args.remove_type == 2: # 删单点，删多次\n",
    "    print(f\"{batch_delete_num} deleted. label of cur image: {train_data[1][batch_delete_num].item()}.\")\n",
    "    \n",
    "    if batch_delete_num not in remove_list: # 若当前点不在list之内，则不必“真”删\n",
    "        continue\n",
    "    else: # 将中间点的data influence 先从matrix中减去\n",
    "        if batch_delete_num > 5: \n",
    "            \n",
    "            index = remove_list.index(batch_delete_num)\n",
    "            pre_index = index - 1\n",
    "            sub_seq = re_sequence[remove_list[pre_index]:remove_list[index]-1] # 从上一个被删除的datapoint开始，到当前被删点的前一个datapoint\n",
    "\n",
    "            # remove matrix and add grad 取两次删除的中间部分这些点，只计算aggregated grad然后更新模型即可，不用“真”的删\n",
    "            temp_loader = make_loader(train_data, batch_size=pass_batch, head=remove_list[pre_index], rear=remove_list[index]-1)\n",
    "            print(f\"temp_loader: train_data[{remove_list[pre_index]},{remove_list[index]-1}]\")\n",
    "            for index, (image, label) in enumerate(temp_loader):\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                # x+delta\n",
    "                image_perturbed = atk(image, label).to(device)\n",
    "                \n",
    "                update_grad(grad, clean_grad, weight, image, image_perturbed, label, feature, args)\n",
    "                update_matrix(matrices, weight, image, image_perturbed, label, feature, public_partial_dd_inv, args, flag='muter')\n",
    "                    \n",
    "#                 # for perturbed grad\n",
    "#                 # :: aggregate the adversarial gradients\n",
    "#                 grad = grad + \\\n",
    "#                     parll_loss_grad(weight, \\\n",
    "#                                     image_perturbed.view(image_perturbed.shape[0], feature), \\\n",
    "#                                     label, args)\n",
    "                \n",
    "#                 # for MUter matrix\n",
    "#                 # :: delete the batch_hessian (data influence)\n",
    "#                 matrix = matrix - \\\n",
    "#                     (batch_hessian(weight, image_perturbed.view(image_perturbed.shape[0], feature), label, args) - \\\n",
    "#                      parll_partial(image_perturbed.view(image_perturbed.shape[0], feature, 1), label, \\\n",
    "#                                    weight, public_partial_dd_inv).sum(dim=0).detach())\n",
    "    \n",
    "    print()\n",
    "    print('The {}-th delete'.format(step))\n",
    "    step = step + 1\n",
    "    # prepare work\n",
    "    unlearning_model = copy.deepcopy(model).to(device)  # for MUter method\n",
    "    # if delete_batch = 20\n",
    "    # batch_delete_num = 20, delete_loader = train_data[0:20]\n",
    "    # batch_delete_num = 40, delete_loader = train_data[20:40]\n",
    "    delete_loader = make_loader(train_data, batch_size=pass_batch, head=(batch_delete_num-delete_batch), rear=batch_delete_num)\n",
    "    print(f\"delete_loader: train_data[{(batch_delete_num-delete_batch)},{batch_delete_num}]\")\n",
    "    \n",
    "    ## retrain_from_scratch\n",
    "    retrain_loader = make_loader(train_data, batch_size=128, head=batch_delete_num)\n",
    "    retrain_model = retrain_from_scratch(retrain_loader, test_loader, args, saver)\n",
    "    \n",
    "    # calculate the aggregated grad & clean_grad\n",
    "    for index, (image, label) in enumerate(delete_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        image_perturbed = atk(image, label).to(device)\n",
    "\n",
    "        update_grad(grad, clean_grad, weight, image, image_perturbed, label, feature, args)\n",
    "    \n",
    "    # unlearning stage\n",
    "    ## MUter\n",
    "    Dww, H_11, H_12, H_21, neg_H_22 = None, None, None, None, None\n",
    "    start_time = time.time()\n",
    "    for index, (image, label) in enumerate(delete_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        image_perturbed = atk(image, label).to(device)\n",
    "\n",
    "        if args.isbatch ==  False:\n",
    "            Dww, H_11, H_12, _, H_21, neg_H_22 = partial_hessian(image_perturbed.view(feature, 1), label, weight, public_partial_dd_inv, args, isUn_inv=True, public_partial_xx=public_partial_dd)\n",
    "        else: # for mini-batch\n",
    "            update_matrix(matrices, weight, image, image_perturbed, label, feature, public_partial_dd_inv, args, flag='muter')\n",
    "    \n",
    "    unlearn_muter(matrices['MUter'], model, grad, Dww, H_11, H_12, H_21, neg_H_22, feature, device, start_time, retrain_model, test_loader, args, saver)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0278ecc",
   "metadata": {},
   "source": [
    "## the golden baseline: retrain_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrain_from_scratch(batch_delete_num):\n",
    "#     # retrain_from_scratch\n",
    "#     retrain_loader = make_loader(train_data, batch_size=128, head=batch_delete_num)\n",
    "#     retrain_model, retrain_time = train(retrain_loader, test_loader, args, verbose=False)    \n",
    "#     clean_acc, perturb_acc = Test_model(retrain_model, test_loader, args)\n",
    "#     print()\n",
    "#     print('Retrain from scratch:')\n",
    "#     print(f\"retrain_loader: train_data[{batch_delete_num}:]\")\n",
    "#     print(f'retrain model test acc: clean_acc {clean_acc}, preturb_acc: {perturb_acc}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4164bbb",
   "metadata": {},
   "source": [
    "# Unlearning methods.\n",
    "## 0. MUter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e108ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MUter(batch_delete_num, delete_loader, matrix, grad, unlearning_model):\n",
    "#     unlearning_time = 0.0 # record one batch spending time for MUter\n",
    "#     # building matrix\n",
    "#     Dww = None\n",
    "#     H_11 = None\n",
    "#     H_12 = None\n",
    "#     H_21 = None\n",
    "#     neg_H_22 = None\n",
    "    \n",
    "#     # Unlearning\n",
    "#     start_time = time.time()\n",
    "#     for index, (image, label) in enumerate(delete_loader):\n",
    "#         image = image.to(device)\n",
    "#         label = label.to(device)\n",
    "#         image_perturbed = atk(image, label).to(device)\n",
    "\n",
    "#         if args.isbatch ==  False: # 删除单点：\n",
    "#             Dww, H_11, H_12, _, H_21, neg_H_22 = partial_hessian(image_perturbed.view(feature, 1), label, \n",
    "#                                                                  weight, public_partial_dd_inv, \n",
    "#                                                                  args, isUn_inv=True, \n",
    "#                                                                  public_partial_xx=public_partial_dd)    \n",
    "#         else: # for mini-batch # 删除多点\n",
    "#             matrix = matrix - \\\n",
    "#                 (batch_hessian(weight, image_perturbed.view(image_perturbed.shape[0], feature), label, args) - \\\n",
    "#                  parll_partial(image_perturbed.view(image_perturbed.shape[0], feature, 1), label, \\\n",
    "#                                weight, public_partial_dd_inv).sum(dim=0).detach())\n",
    "\n",
    "#         grad = grad + \\\n",
    "#             parll_loss_grad(weight, \\\n",
    "#                             image_perturbed.view(image_perturbed.shape[0], feature), \\\n",
    "#                             label, args)\n",
    "\n",
    "#     if args.isbatch == False:\n",
    "#         block_matrix = buliding_matrix(matrix, H_11, H_12, -neg_H_22, H_21)\n",
    "#         print('block_matrix shape {}'.format(block_matrix.shape))\n",
    "#         grad_cat_zero = torch.cat([grad, torch.zeros((feature, 1)).to(device)], dim=0)\n",
    "#         print('grad_cat_zeor shape {}'.format(grad_cat_zero.shape))\n",
    "\n",
    "#         delta_w_cat_alpha = cg_solve(block_matrix, grad_cat_zero.squeeze(dim=1), get_iters(args))\n",
    "#         delta_w = delta_w_cat_alpha[:feature]\n",
    "\n",
    "#         update_w(delta_w, unlearning_model)\n",
    "#         matrix = matrix - Dww\n",
    "#     else:\n",
    "#         delta_w = cg_solve(matrix, grad.squeeze(dim=1), get_iters(args))\n",
    "#         update_w(delta_w, unlearning_model)\n",
    "    \n",
    "#     clean_acc, perturb_acc = Test_model(unlearning_model, test_loader, args) \n",
    "#     model_dist = model_distance(retrain_model, unlearning_model)\n",
    "#     print()\n",
    "#     print('MUter unlearning:')\n",
    "#     print(f'unlearning model test acc: clean_acc {clean_acc}, preturb_acc: {perturb_acc}')\n",
    "#     print('model distance between Muter and retrain_from_scratch: {:.4f}'.format(model_dist))\n",
    "    \n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
